#include "SILC_Mpi.h"
#include "config.h"

/**
 * @file       SILC_Mpi_Env.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     ALPHA
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for environmental management.
 */

/** Flag set if the measurement sysem was already opened by another adapter.
    If the measurement system is not already initilized, it is assumed that
    the mpi adapter is the only active adapter. In this case, at first an
    additional region is entered MPI_Init. Thus, all regions appear as
    childs of this region.
*/
static int silc_mpi_parallel_entered = 0;

/**
 * @name C wrappers
 * @{
 */

/**
 * Measurement wrapper for MPI_Init.
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup env
 * If the measurement system is not initialized, it will iniialize the measurement
 * system and enter a special region named "parallel" which is exited when MPI is 
 * finalized. 
 * The sequence of events generated by this wrapper is:
 * @li enter region 'PARALLEL': Only if this adapter initializes the measurement system.
 * @li enter region 'MPI_Init'
 * @li define communicator 'COMM_WORLD'
 * @li exit region 'MPI_Init'
 */
int MPI_Init(int* argc, char*** argv )
{
  int            event_gen_active = 0; /* init is deferred to later */
  int            return_val, i;
  int            fflag, rank;

  if (! SILC_IsInitialized())
  {
    /* Initialize the measurement system */
    SILC_InitMeasurement();

    /* Enter global MPI region */
    SILC_EnterRegion(silc_mpi_regid[SILC_PARALLEL__MPI]);

    /* Remember that SILC_PARALLEL__MPI was entered */
    silc_mpi_parallel_entered = 1;
  }

  event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_ENV);

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    /* Enter the init region */
    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_INIT]);
  }

  return_val = PMPI_Init(argc, argv);

  /* XXXX should only continue if MPI initialised OK! */

  if ((PMPI_Finalized(&fflag) == MPI_SUCCESS) && (fflag == 0))
  {
    /* initialize communicator management and register MPI_COMM_WORLD*/
    silc_mpi_comm_init();

    /* Obtain rank */
    PMPI_Comm_rank(MPI_COMM_WORLD, &rank);

    /* complete initialization of measurement core and MPI event handling */
    SILC_InitMeasurementMPI(rank);
  }

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_INIT]);
    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}

#if HAVE(DECL_PMPI_INIT_THREAD)
/**
 * Measurement wrapper for MPI_Init_thread, the thread-capable
 * alternative to MPI_Init.
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup env
 * If the measurement system is not initialized, it will iniialize the measurement
 * system and enter a special region named "parallel" which is exited when MPI is 
 * finalized. 
 * The sequence of events generated by this wrapper is:
 * @li enter region 'PARALLEL': Only if this adapter initializes the measurement system.
 * @li enter region 'MPI_Init_thread'
 * @li define communicator 'COMM_WORLD'
 * @li exit region 'MPI_Init_thread'
 */
int MPI_Init_thread(int* argc, char*** argv, int required, int* provided )
{
  int            event_gen_active = 0;
  int            return_val, i;
  int            fflag, rank;

  if (! SILC_IsInitialized())
  {
    /* Initialize the measurement system */
    SILC_InitMeasurement();

    /* Enter global MPI region */
    SILC_EnterRegion(silc_mpi_regid[SILC_PARALLEL__MPI]);

    /* Remember that SILC_PARALLEL__MPI was entered */
    silc_mpi_parallel_entered = 1;
  }

  event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_ENV);

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_INIT_THREAD]);
  }

  return_val = PMPI_Init_thread(argc, argv, required, provided);

  /* XXXX should only continue if MPI initialised OK! */

  if ((return_val == MPI_SUCCESS) && (*provided > MPI_THREAD_FUNNELED))
  {
    SILC_DEBUG_PRINTF( SILC_WARNING | SILC_DEBUG_MPI ,
      "MPI environment initialized with level exceeding MPI_THREAD_FUNNELED!");
    /* XXXX continue even though not supported by analysis */
  }

  if ((PMPI_Finalized(&fflag) == MPI_SUCCESS) && (fflag == 0))
  {
    /* initialize communicator management and register MPI_COMM_WORLD */
    silc_mpi_comm_init();

    /* Obtain rank */
    PMPI_Comm_rank(MPI_COMM_WORLD, &rank);

    /* complete initialization of measurement core and MPI event handling */
    SILC_InitMeasurementMPI(rank);
  }

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_INIT_THREAD]);
    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

/**
 * Measurement wrapper for MPI_Finalize
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup env
 * Generates an enter event at function start and an exit event at function end.
 * If the "parallel" region was entered in MPI_Init, it exits the "parallel" region.
 * It does not perform the MPI finalization, because MPI is still needed by the
 * measurement system, but substituts it with a barrier. The MPI finalization
 * will be done from the measurement system. 
 */
int MPI_Finalize()
{
  const int event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_ENV);
  int       return_val;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();
    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_FINALIZE]);
  }

  /* finalize communicator and request management */
  silc_mpi_comm_finalize();
/* Asynchroneous communication not supported
  silc_mpi_request_finalize();
*/

  /* finalize MPI event handling */
  /* esd_mpi_finalize(); */

  /* fake finalization, so that MPI can be used during EPIK finalization */
  return_val = PMPI_Barrier(MPI_COMM_WORLD);

  if (event_gen_active)
  {
    /* Exit MPI_Finalize region */
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_FINALIZE]);

    /* Exit the extra parallel region in case it was entered in MPI_Init */
    if (silc_mpi_parallel_entered)
    {
      SILC_ExitRegion(silc_mpi_regid[SILC_PARALLEL__MPI]);
    }

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}

#pragma wrapgen multiple restrict(genv+!nMPI_Init+!nMPI_Init_thread+!nMPI_Finalize) skel/SILC_Mpi_Std.w

#pragma wrapgen multiple regex((MPI_Initialized|MPI_Finalized)) skel/SILC_Mpi_Std.w 

/**
 * @}
 */
