#include "SILC_Mpi.h"
#include "config.h"

/**
 * @file  SILC_Mpi_P2p.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     ALPHA
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for point-to-point communication
 */

/**
 * @name Blocking
 * @{
 */
#pragma wrapgen multiple regex(MPI_(S|B|R)[s]?end$) skel/SILC_Mpi_PtpSend.w

#if HAVE(DECL_PMPI_RECV) && !defined(SILC_MPI_NO_P2P)
/** 
 * Measurement wrapper for MPI_Recv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int MPI_Recv( void* buf,
              int count,
              MPI_Datatype datatype,
              int source, int tag,
              MPI_Comm comm,
              MPI_Status* status )
{
  int return_val;

  if (SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P))
  {
    int        sz;
    MPI_Status mystatus;

    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_RECV]);

    if (status == MPI_STATUS_IGNORE) status = &mystatus;
    return_val = PMPI_Recv(buf, count, datatype, source, tag, comm, status);

    if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
      PMPI_Type_size(datatype, &sz);
      PMPI_Get_count(status, datatype, &count);
      SILC_MpiRecv(SILC_MPI_RANK_TO_PE(status->MPI_SOURCE, comm),
                   SILC_MPI_COMM_ID(comm), status->MPI_TAG, count * sz);
    }

    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_RECV]);

    SILC_MPI_EVENT_GEN_ON();
  }
  else
  {
    return_val = PMPI_Recv(buf, count, datatype, source, tag, comm, status);
  }

  return return_val;
}
#endif

#pragma wrapgen single MPI_Probe skel/SILC_Mpi_Std.w

#if HAVE(DECL_PMPI_SENDRECV) && !defined(SILC_MPI_NO_P2P)
/** 
 * Measurement wrapper for MPI_Sendrecv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI send event
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int MPI_Sendrecv(void* sendbuf,
                 int sendcount,
                 MPI_Datatype sendtype,
                 int dest,
                 int sendtag,
                 void* recvbuf,
                 int recvcount,
                 MPI_Datatype recvtype,
                 int source,
                 int recvtag,
                 MPI_Comm comm,
                 MPI_Status* status )
{
  int return_val;

  if (SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P))
  {
    int        sendsz, recvsz;
    MPI_Status mystatus;

    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_SENDRECV]);

    if (dest != MPI_PROC_NULL)
    {
      PMPI_Type_size(sendtype, &sendsz);
      SILC_MpiSend(SILC_MPI_RANK_TO_PE(dest, comm), SILC_MPI_COMM_ID(comm),
                   sendtag, sendcount * sendsz);
    }

    if (status == MPI_STATUS_IGNORE)
    {
      status = &mystatus;
    }

    return_val = PMPI_Sendrecv(sendbuf, sendcount, sendtype, dest,   sendtag,
                           recvbuf, recvcount, recvtype, source, recvtag,
                           comm, status);
    if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
      {
        PMPI_Type_size(recvtype, &recvsz);
        PMPI_Get_count(status, recvtype, &recvcount);
        SILC_MpiRecv(SILC_MPI_RANK_TO_PE(status->MPI_SOURCE, comm),
                     SILC_MPI_COMM_ID(comm), status->MPI_TAG, recvcount * recvsz);
      }

    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_SENDRECV]);

    SILC_MPI_EVENT_GEN_ON();
  }
  else
  {
    return_val = PMPI_Sendrecv(sendbuf, sendcount, sendtype, dest,   sendtag,
                           recvbuf, recvcount, recvtype, source, recvtag,
                           comm, status);
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_SENDRECV_REPLACE) && !defined(SILC_MPI_NO_P2P)
/** 
 * Measurement wrapper for MPI_Sendrecv_replace
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI send event
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int MPI_Sendrecv_replace(void* buf,
                         int count,
                         MPI_Datatype datatype,
                         int dest,
                         int sendtag,
                         int source,
                         int recvtag,
                         MPI_Comm comm,
                         MPI_Status* status )
{
  int return_val;


  if (SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P))
  {
    int        sz;
    MPI_Status mystatus;

    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_SENDRECV_REPLACE]);

    PMPI_Type_size(datatype, &sz);
    if (dest != MPI_PROC_NULL)
    {
      SILC_MpiSend(SILC_MPI_RANK_TO_PE(dest, comm),
                   SILC_MPI_COMM_ID(comm),
                   sendtag,
                   count * sz);
    }

    if (status == MPI_STATUS_IGNORE)
    {
      status = &mystatus;
    }
    
    return_val = PMPI_Sendrecv_replace(buf, count, datatype, dest,
                                   sendtag, source, recvtag,
                                   comm, status);
    if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
      SILC_MpiRecv(SILC_MPI_RANK_TO_PE(status->MPI_SOURCE, comm),
                   SILC_MPI_COMM_ID(comm), status->MPI_TAG, count * sz);
    }

    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_SENDRECV_REPLACE]);

    SILC_MPI_EVENT_GEN_ON();
  }
  else
  {
    return_val = PMPI_Sendrecv_replace(buf, count, datatype, dest,
                                   sendtag, source, recvtag,
                                   comm, status);
  }

  return return_val;
}
#endif

/**
 * @}
 * @name Non-blocking
 * @{
 */

#pragma wrapgen multiple regex(MPI_I(s|bs|rs|ss)end$) skel/SILC_Mpi_PtpIsend.w

#if HAVE(DECL_PMPI_IRECV) && !defined(SILC_MPI_NO_P2P)
/** 
 * Measurement wrapper for MPI_Irecv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Irecv(void* buf,
              int count,
              MPI_Datatype datatype,
              int source,
              int tag,
              MPI_Comm comm,
              MPI_Request* request)
{
  const int event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int       return_val;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_IRECV]);
  }

  return_val = PMPI_Irecv(buf, count, datatype, source, tag, comm, request);
 
/* no asynchroneous communication handling at first
 if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
  {
    uint32_t reqid = silc_get_request_id();
    int sz;
    PMPI_Type_size(datatype, &sz);

    if (event_gen_active && xnb_active)
      silc_mpi_recv_request(reqid);

    silc_request_create(*request, ERF_RECV,
                       tag, 0, count * sz, datatype, comm, regid);
  }
*/

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_IRECV]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#pragma wrapgen single MPI_Iprobe skel/SILC_Mpi_Std.w

#if HAVE(DECL_PMPI_WAIT) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Wait
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Wait(MPI_Request* request,
             MPI_Status* status)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  int                return_val;
  MPI_Status         mystatus;
  struct SilcRequest* orig_req;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_WAIT]);
  }

  if (status == MPI_STATUS_IGNORE)
  {
    status = &mystatus;
  }

/* no asynchroneous communication handling at the beginning included 
  orig_req   = silc_request_get(*request);
*/
  return_val = PMPI_Wait(request, status);

/* no asynchroneous communication handling at the beginning included 
  silc_check_request(orig_req, status);
*/

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_WAIT]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_WAITALL) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Waitall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Waitall(int count,
                MPI_Request* requests,
                MPI_Status* array_of_statuses)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  int                return_val, i;
  struct SilcRequest* orig_req;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_WAITALL]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (array_of_statuses == MPI_STATUSES_IGNORE) 
  {
    array_of_statuses = silc_get_status_array(count);
  }
  silc_save_request_array(requests, count);
*/
  return_val = PMPI_Waitall(count, requests, array_of_statuses);
/* no asynchroneous communication handling at the beginning included 
  for (i = 0; i < count; i++)
  {
    orig_req = silc_saved_request_get(i);
    silc_check_request(orig_req, &(array_of_statuses[i]));
  }
*/
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_WAITALL]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_WAITANY) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Waitany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Waitany(int count,
                MPI_Request* requests,
                int* index,
                MPI_Status* status)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val;
  struct SilcRequest* orig_req;
  MPI_Status         mystatus;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_WAITANY]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (status == MPI_STATUS_IGNORE)
  {
    status = &mystatus;
  }

  silc_save_request_array(requests, count);
*/
  return_val = PMPI_Waitany(count, requests, index, status);

/* no asynchroneous communication handling at the beginning included 
  if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; ++i) {
        orig_req = silc_saved_request_get(i);

        if (i == *index)
          silc_check_request(orig_req, status);
        else if (orig_req)
          silc_mpi_request_tested(orig_req->id);
      }
    }
  else
    {
      orig_req   = silc_saved_request_get(*index);
      silc_check_request(orig_req, status);
    }

*/  
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_WAITANY]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_WAITSOME) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Waitsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Waitsome(int incount,
                 MPI_Request *array_of_requests,
                 int *outcount,
                 int *array_of_indices,
                 MPI_Status *array_of_statuses)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val, i;
  struct SilcRequest* orig_req;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_WAITSOME]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (array_of_statuses == MPI_STATUSES_IGNORE)
  {
    array_of_statuses = silc_get_status_array(incount);
  }
  silc_save_request_array(array_of_requests, incount);
*/
  return_val = PMPI_Waitsome(incount, array_of_requests, outcount,
                         array_of_indices, array_of_statuses );
/* no asynchroneous communication handling at the beginning included 
  if (event_gen_active && xnb_active)
    {
      int j, tmp, cur;
      MPI_Status tmpstat;

      cur = 0;

      for (i = 0; i < incount; ++i)
        {
          orig_req = silc_saved_request_get(i);

          if (orig_req)
            {
              for (j = cur; j < *outcount && i != array_of_indices[j]; ++j)
                ;

              if (j < *outcount)
                {
                  tmpstat               = array_of_statuses[cur];
                  silc_check_request(orig_req, &(array_of_statuses[cur]));
                  array_of_statuses[j]  = tmpstat;

                  tmp                   = array_of_indices[cur];
                  array_of_indices[cur] = array_of_indices[j];
                  array_of_indices[j]   = tmp;

                  ++cur;
                }
              else
                {
                  silc_mpi_request_tested(orig_req->id);
                }
            }
        }
    }
  else
    {
      for (i=0; i<*outcount; ++i)
        {
          orig_req = silc_saved_request_get(array_of_indices[i]);
          silc_check_request(orig_req, &(array_of_statuses[i]));
        }
    }

*/

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_WAITSOME]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_TEST) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Test
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Test(MPI_Request* request,
             int* flag,
             MPI_Status* status)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val;
  struct SilcRequest* orig_req;
  MPI_Status         mystatus;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_TEST]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (status == MPI_STATUS_IGNORE)
  {
    status = &mystatus;
  }
  orig_req   = silc_request_get(*request);
*/
  return_val = PMPI_Test(request, flag, status);
/* no asynchroneous communication handling at the beginning included 
  if (*flag) 
    {
      silc_check_request(orig_req, status);
    }
  else if (orig_req && event_gen_active && xnb_active)
    {
      silc_mpi_request_tested(orig_req->id);
    }
*/
  
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_TEST]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_TESTANY) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Testany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Testany(int count,
                MPI_Request *array_of_requests,
                int *index,
                int *flag,
                MPI_Status *status)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val;
  struct SilcRequest* orig_req;
  MPI_Status         mystatus;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_TESTANY]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (status == MPI_STATUS_IGNORE)
  {
    status = &mystatus;
  }
  silc_save_request_array(array_of_requests, count);
*/
  return_val = PMPI_Testany( count, array_of_requests, index, flag, status );

/* no asynchroneous communication handling at the beginning included 
  if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; ++i) {
        orig_req = silc_saved_request_get(i);

        if (*index == i)
          silc_check_request(orig_req, status);
        else if (orig_req)
          silc_mpi_request_tested(orig_req->id);
      }
    }
  else if (*flag && *index != MPI_UNDEFINED)
    {
      orig_req = silc_saved_request_get(*index);
      silc_check_request(orig_req, status);
    }
*/  
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_TESTANY]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_TESTALL) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Testall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Testall(int count,
                MPI_Request *array_of_requests,
                int *flag,
                MPI_Status *array_of_statuses)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val, i;
  struct SilcRequest* orig_req;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_TESTALL]);
  }
/* no asynchroneous communication handling at the beginning included 
  if (array_of_statuses == MPI_STATUSES_IGNORE)
  {
    array_of_statuses = silc_get_status_array(count);
  }
  silc_save_request_array(array_of_requests, count);
*/
  return_val = PMPI_Testall(count, array_of_requests, flag, array_of_statuses);
/* no asynchroneous communication handling at the beginning included 
  if (*flag)
    {
      for (i = 0; i < count; i++)
        {
          orig_req = silc_saved_request_get(i);
          silc_check_request(orig_req, &(array_of_statuses[i]));
        }
    }
  else if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; i++)
        {
          orig_req = silc_saved_request_get(i);
          if (orig_req)
            silc_mpi_request_tested(orig_req->id);
        }
    }
*/
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_TESTALL]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_TESTSOME) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Testsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Testsome(int incount,
                 MPI_Request *array_of_requests,
                 int *outcount,
                 int *array_of_indices,
                 MPI_Status *array_of_statuses)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val, i;
  struct SilcRequest* orig_req;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_TESTSOME]);
  }

/* no asynchroneous communication handling at the beginning included 
  if (array_of_statuses == MPI_STATUSES_IGNORE)
  {
    array_of_statuses = silc_get_status_array(incount);
  }
  silc_save_request_array(array_of_requests, incount);
*/
  return_val = PMPI_Testsome( incount, array_of_requests, outcount,
                          array_of_indices, array_of_statuses );

/* no asynchroneous communication handling at the beginning included 
  if (event_gen_active && xnb_active)
    {
      int cur, j, tmp;
      MPI_Status tmpstat;

      cur = 0;

      for (i=0; i<incount; ++i)
        {
          orig_req = silc_saved_request_get(i);

          if (orig_req)
            {
              for (j = cur; j < *outcount && i != array_of_indices[j]; ++j)
                ;

              if (j < *outcount)
                {
                  tmpstat               = array_of_statuses[cur];
                  silc_check_request(orig_req, &(array_of_statuses[cur]));
                  array_of_statuses[j]  = tmpstat;

                  tmp                   = array_of_indices[cur];
                  array_of_indices[cur] = array_of_indices[j];
                  array_of_indices[j]   = tmp;

                  ++cur;
                }
              else
                {
                  silc_mpi_request_tested(orig_req->id);
                }
            }
        }
    }
  else
    {
      for (i=0; i<*outcount; ++i)
        {
          orig_req = silc_saved_request_get(array_of_indices[i]);
          silc_check_request(orig_req, &(array_of_statuses[i]));
        }
    }

*/
  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_TESTSOME]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

/**
 * @}
 * @name Persitent requests
 * @{
 */

/* no asynchroneous communication handling at the beginning included 
pragma wrapgen multiple regex(MPI_(S|B|R)[s]?end_init$) skel/SILC_Mpi_PtpSendinit.w
*/
#pragma wrapgen multiple regex(MPI_(S|B|R)[s]?end_init$) skel/SILC_Mpi_Std.w

#if HAVE(DECL_PMPI_RECV_INIT) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Recv_init
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Recv_init(void* buf,
                  int count,
                  MPI_Datatype datatype,
                  int source,
                  int tag,
                  MPI_Comm comm,
                  MPI_Request* request)
{
  const int event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  int       return_val;

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_RECV_INIT]);
  }

  return_val = PMPI_Recv_init(buf, count, datatype, source, tag, comm, request);
/* no asynchroneous communication handling at the beginning included 
  if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
  {
    int sz;
    PMPI_Type_size(datatype, &sz);
    silc_request_create(*request, (ERF_RECV | ERF_IS_PERSISTENT),
                       tag, source, count * sz, datatype, comm,
                       silc_get_request_id());
  }
*/

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_RECV_INIT]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_START) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Start
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Start(MPI_Request* request)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val;

  if (event_gen_active)
  {
    struct SilcRequest* req;

    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_START]);

/* no asynchroneous communication handling at the beginning included 
    req = silc_request_get(*request);
    if (req && (req->flags & ERF_IS_PERSISTENT))
      {
        req->flags |= ERF_IS_ACTIVE;
        if ((req->flags & ERF_SEND) && (req->dest != MPI_PROC_NULL))
          {
            if (xnb_active)
              silc_attr_ui4(ELG_ATTR_REQUEST, req->id);

            SILC_MpiSend(SILC_MPI_RANK_TO_PE(req->dest, req->comm),
                         SILC_COMM_ID(req->comm), req->tag,  req->bytes);
          }
        else if (req->flags & ERF_RECV && xnb_active)
          {
            silc_mpi_recv_request(req->id);
          }
      }
*/
  }

  return_val = PMPI_Start(request);

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_START]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_STARTALL) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Startall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Startall(int count,
                 MPI_Request *array_of_requests)
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                return_val, i;

  if (event_gen_active)
  {
    MPI_Request*       request;

/* no asynchroneous communication handling at the beginning included 
    struct SilcRequest* req;
*/

    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_STARTALL]);

/* no asynchroneous communication handling at the beginning included 
    for (i = 0; i < count; i++)
    {
      request = &array_of_requests[i];
      req     = silc_request_get(*request);
      
      if (req && (req->flags & ERF_IS_PERSISTENT))
        {
          req->flags |= ERF_IS_ACTIVE;
          if ((req->flags & ERF_SEND) && (req->dest != MPI_PROC_NULL))
            {
              if (xnb_active)
                silc_attr_ui4(ELG_ATTR_REQUEST, req->id);

              SILC_MpiSend(SILC_MPI_RANK_TO_PE(req->dest, req->comm),
                           SILC_COMM_ID(req->comm), req->tag,  req->bytes);
            }
          else if (req->flags & ERF_RECV && xnb_active)
            {
              silc_mpi_recv_request(req->id);
            }
        }

    }
*/
  }

  return_val = PMPI_Startall( count, array_of_requests );

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_STARTALL]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_REQUEST_FREE) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Request_free
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Request_free( MPI_Request* request )
{
  const int          event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  const int          xnb_active       = (silc_mpi_enabled & SILC_MPI_ENABLED_XNONBLOCK);
  int                orig_req_null    = (*request == MPI_REQUEST_NULL);
  int                return_val;
/* no asynchroneous communication handling at the beginning included 
  struct SilcRequest* req;
*/

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_REQUEST_FREE]);
  }

/* no asynchroneous communication handling at the beginning included 
  req = silc_request_get(*request);
  if (req)
    {
      if (req->flags & ERF_CAN_CANCEL && event_gen_active && xnb_active)
        {
          MPI_Status status;
          int        cancelled;
*/
          /* -- Must check if request was cancelled and write the 
           *    cancel event. Not doing so will confuse the trace 
           *    analysis.
           */
/*
          return_val = PMPI_Wait(request, &status);
          PMPI_Test_cancelled(&status, &cancelled);

          if (cancelled)
            esd_mpi_cancelled(req->id);
        }

      if ((req->flags & ERF_IS_PERSISTENT) && (req->flags & ERF_IS_ACTIVE))
*/        /* mark active requests for deallocation */
/*        req->flags |= ERF_DEALLOCATE;
      else
*/        /* deallocate inactive requests -*/
/*        epk_request_free(req);
    }
*/
  /* -- We had to call PMPI_Wait for cancellable requests, which already
   *    frees (non-persistent) requests itself and sets them to 
   *    MPI_REQUEST_NULL. 
   *    As MPI_Request_free does not really like being called with 
   *    MPI_REQUEST_NULL, we have to catch this situation here and only 
   *    pass MPI_REQUEST_NULL if the application explicitely wanted that 
   *    for some reason.
   */
  if (*request != MPI_REQUEST_NULL || orig_req_null)
    return_val = PMPI_Request_free(request);


  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_REQUEST_FREE]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#if HAVE(DECL_PMPI_CANCEL) && !defined(SILC_MPI_NO_P2P)
/**
 * Measurement wrapper for MPI_Cancel
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int MPI_Cancel( MPI_Request* request )
{
  const int event_gen_active = SILC_MPI_IS_EVENT_GEN_ON_FOR(SILC_MPI_ENABLED_P2P);
  int       return_val;
/* no asynchroneous communication handling at the beginning included 
  struct SilcRequest* req;
*/

  if (event_gen_active)
  {
    SILC_MPI_EVENT_GEN_OFF();

    SILC_EnterRegion(silc_mpi_regid[SILC__MPI_CANCEL]);
  }

  /* Mark request as cancellable and check for successful cancellation
   * on request completion or MPI_Request_free.
   * If XNONBLOCK is enabled, there will be a 'cancelled' event 
   * instead of a normal completion event in the trace, which can be
   * checked for by the trace analysis.
   */

/* no asynchroneous communication handling at the beginning included 
  req = silc_request_get(*request);

  if (req)
    req->flags |= ERF_CAN_CANCEL;
*/
  return_val = PMPI_Cancel(request);

  if (event_gen_active)
  {
    SILC_ExitRegion(silc_mpi_regid[SILC__MPI_CANCEL]);

    SILC_MPI_EVENT_GEN_ON();
  }

  return return_val;
}
#endif

#pragma wrapgen single MPI_Test_cancelled skel/SILC_Mpi_Std.w

/**
 * @}
 * @name Auxiluary functions
 * @{
 */

#pragma wrapgen multiple regex((Buffer_attach|Buffer_detach)) skel/SILC_Mpi_Std.w

/**
 * @}
 */

