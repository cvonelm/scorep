/*
 * This file is part of the Score-P software (http://www.score-p.org)
 *
 * Copyright (c) 2009-2011,
 *    RWTH Aachen University, Germany
 *    Gesellschaft fuer numerische Simulation mbH Braunschweig, Germany
 *    Technische Universitaet Dresden, Germany
 *    University of Oregon, Eugene, USA
 *    Forschungszentrum Juelich GmbH, Germany
 *    German Research School for Simulation Sciences GmbH, Juelich/Aachen, Germany
 *    Technische Universitaet Muenchen, Germany
 *
 * See the COPYING file in the package base directory for details.
 *
 */


/**
 * @file  SCOREP_Mpi_Coll.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     alpha
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for collective communication
 */

#include <config.h>
#include "SCOREP_Mpi.h"

/**
 * @name C wrappers
 * @{
 */

#if HAVE( DECL_PMPI_ALLGATHER )
/**
 * Measurement wrapper for MPI_Allgather
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgather( void*        sendbuf,
               int          sendcount,
               MPI_Datatype sendtype,
               void*        recvbuf,
               int          recvcount,
               MPI_Datatype recvtype,
               MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvsz, sendsz, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLGATHER ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLGATHER ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * sendcount * sendsz,
                              N * recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLGATHER ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLGATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allgatherv )
/**
 * Measurement wrapper for MPI_Allgatherv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgatherv( void*        sendbuf,
                int          sendcount,
                MPI_Datatype sendtype,
                void*        recvbuf,
                int*         recvcounts,
                int*         displs,
                MPI_Datatype recvtype,
                MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvcount, recvsz, sendsz, i, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLGATHERV ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_size( comm, &N );
        recvcount = 0;
        for ( i = 0; i < N; i++ )
        {
            recvcount += recvcounts[ i ];
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLGATHERV ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * sendcount * sendsz,
                              recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLGATHERV ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLREDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allreduce )
/**
 * Measurement wrapper for MPI_Allreduce
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allreduce( void*        sendbuf,
               void*        recvbuf,
               int          count,
               MPI_Datatype datatype,
               MPI_Op       op,
               MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sz, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLREDUCE ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLREDUCE ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * count * sz,
                              N * count * sz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLREDUCE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoall )
/**
 * Measurement wrapper for MPI_Alltoall
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoall( void*        sendbuf,
              int          sendcount,
              MPI_Datatype sendtype,
              void*        recvbuf,
              int          recvcount,
              MPI_Datatype recvtype,
              MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvsz, sendsz, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALL ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLTOALL ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * sendcount * sendsz,
                              N * recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALL ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALLV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallv )
/**
 * Measurement wrapper for MPI_Alltoallv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallv( void*        sendbuf,
               int*         sendcounts,
               int*         sdispls,
               MPI_Datatype sendtype,
               void*        recvbuf,
               int*         recvcounts,
               int*         rdispls,
               MPI_Datatype recvtype,
               MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvcount = 0, sendcount = 0, recvsz, sendsz, N, i;
        SCOREP_Mpi_Rank root_loc  = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLV ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_size( comm, &N );
        for ( i = 0; i < N; i++ )
        {
            recvcount += recvcounts[ i ];
            sendcount += sendcounts[ i ];
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLV ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              sendcount * sendsz,
                              recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLV ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ALLTOALLW ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallw )
/**
 * Measurement wrapper for MPI_Alltoallw
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallw( void*        sendbuf,
               int          sendcounts[],
               int          sdispls[],
               MPI_Datatype sendtypes[],
               void*        recvbuf,
               int          recvcounts[],
               int          rdispls[],
               MPI_Datatype recvtypes[],
               MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvcount = 0, sendcount = 0, recvsz, sendsz, N, i;
        SCOREP_Mpi_Rank root_loc  = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLW ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Comm_size( comm, &N );
        for ( i = 0; i < N; i++ )
        {
            PMPI_Type_size( recvtypes[ i ], &recvsz );
            PMPI_Type_size( sendtypes[ i ], &sendsz );
            recvcount += recvsz * recvcounts[ i ];
            sendcount += sendsz * sendcounts[ i ];
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLW ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              sendcount,
                              recvcount );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ALLTOALLW ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_BARRIER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Barrier )
/**
 * Measurement wrapper for MPI_Barrier
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Barrier( MPI_Comm comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BARRIER ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Barrier( comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Barrier( comm, start_time_stamp, return_val );
        }
      #endif

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_BARRIER ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              0,
                              0 );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BARRIER ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Barrier( comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_BCAST ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Bcast )
/**
 * Measurement wrapper for MPI_Bcast
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Bcast( void*        buffer,
           int          count,
           MPI_Datatype datatype,
           int          root,
           MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sz, N, me;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BCAST ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Bcast( buffer, count, datatype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Bcast( buffer, count, datatype, root, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
        }
        else
        {
            N = 0;
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_BCAST ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * count * sz,
                              count * sz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BCAST ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Bcast( buffer, count, datatype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_EXSCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Exscan )
/**
 * Measurement wrapper for MPI_Exscan
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Exscan( void*        sendbuf,
            void*        recvbuf,
            int          count,
            MPI_Datatype datatype,
            MPI_Op       op,
            MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sz, me, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_EXSCAN ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_EXSCAN ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              ( N - me - 1 ) * sz * count,
                              me * sz * count );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_EXSCAN ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_GATHER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gather )
/**
 * Measurement wrapper for MPI_Gather
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gather( void*        sendbuf,
            int          sendcount,
            MPI_Datatype sendtype,
            void*        recvbuf,
            int          recvcount,
            MPI_Datatype recvtype,
            int          root,
            MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sendsz, recvsz, N, me;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_GATHER ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );
        }
        else
        {
            N = recvsz = 0;
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_GATHER ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              sendcount * sendsz,
                              N * recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_GATHER ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_GATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gatherv )
/**
 * Measurement wrapper for MPI_Gatherv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gatherv( void*        sendbuf,
             int          sendcount,
             MPI_Datatype sendtype,
             void*        recvbuf,
             int*         recvcounts,
             int*         displs,
             MPI_Datatype recvtype,
             int          root,
             MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         recvsz, sendsz, recvcount = 0, me, N, i;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_GATHERV ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( sendtype, &sendsz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );
            for ( i = 0; i < N; i++ )
            {
                recvcount += recvcounts[ i ];
            }
        }
        else
        {
            recvcount = recvsz = 0;
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_GATHERV ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              sendcount * sendsz,
                              recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_GATHERV ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce )
/**
 * Measurement wrapper for MPI_Reduce
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce( void*        sendbuf,
            void*        recvbuf,
            int          count,
            MPI_Datatype datatype,
            MPI_Op       op,
            int          root,
            MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sz, me, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_REDUCE ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              count * sz,
                              ( root == me ? N * count * sz : 0 ) );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter )
/**
 * Measurement wrapper for MPI_Reduce_scatter
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter( void*        sendbuf,
                    void*        recvbuf,
                    int*         recvcounts,
                    MPI_Datatype datatype,
                    MPI_Op       op,
                    MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         i, sz, me, N, count = 0;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );
        for ( i = 0; i < N; i++ )
        {
            count += recvcounts[ i ];
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              count * sz,
                              N * recvcounts[ me ] * sz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_REDUCE_SCATTER_BLOCK ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter_block )
/**
 * Measurement wrapper for MPI_Reduce_scatter_block
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter_block( void*        sendbuf,
                          void*        recvbuf,
                          int          recvcount,
                          MPI_Datatype datatype,
                          MPI_Op       op,
                          MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int             sz, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER_BLOCK ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER_BLOCK ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * recvcount * sz,
                              N * recvcount * sz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_SCATTER_BLOCK ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scan )
/**
 * Measurement wrapper for MPI_Scan
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scan( void*        sendbuf,
          void*        recvbuf,
          int          count,
          MPI_Datatype datatype,
          MPI_Op       op,
          MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sz, me, N;
        SCOREP_Mpi_Rank root_loc = SCOREP_INVALID_ROOT_RANK;


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SCAN ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Scan( sendbuf, recvbuf, count, datatype, op, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scan( sendbuf, recvbuf, count, datatype, op, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( datatype, &sz );
        PMPI_Comm_rank( comm, &me );
        PMPI_Comm_size( comm, &N );

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_SCAN ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              ( N - me ) * count * sz,
                              ( me + 1 ) * count * sz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SCAN ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scan( sendbuf, recvbuf, count, datatype, op, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatter )
/**
 * Measurement wrapper for MPI_Scatter
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatter( void*        sendbuf,
             int          sendcount,
             MPI_Datatype sendtype,
             void*        recvbuf,
             int          recvcount,
             MPI_Datatype recvtype,
             int          root,
             MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sendsz, recvsz, N, me;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SCATTER ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif


        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( sendtype, &sendsz );
        }
        else
        {
            N = sendsz = 0;
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_SCATTER ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              N * sendcount * sendsz,
                              recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SCATTER ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SCATTERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatterv )
/**
 * Measurement wrapper for MPI_Scatterv
 * @note Auto-generated by wrapgen from template: coll.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatterv( void*        sendbuf,
              int*         sendcounts,
              int*         displs,
              MPI_Datatype sendtype,
              void*        recvbuf,
              int          recvcount,
              MPI_Datatype recvtype,
              int          root,
              MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        int32_t         sendcount, sendsz, recvsz, me, N, i;
        SCOREP_Mpi_Rank root_loc = SCOREP_MPI_RANK_TO_PE( root, comm );


        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SCATTERV ] );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
      #endif

        return_val = PMPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm );

      #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm, start_time_stamp, return_val );
        }
      #endif


        sendcount = sendsz = 0;
        PMPI_Type_size( recvtype, &recvsz );
        PMPI_Comm_rank( comm, &me );
        if ( me == root )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( sendtype, &sendsz );
            for ( i = 0; i < N; i++ )
            {
                sendcount += sendcounts[ i ];
            }
        }

        SCOREP_MpiCollective( scorep_mpi_regid[ SCOREP__MPI_SCATTERV ],
                              SCOREP_MPI_COMM_ID( comm ),
                              root_loc,
                              sendcount * sendsz,
                              recvcount * recvsz );
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SCATTERV ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_REDUCE_LOCAL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_local )
/**
 * Measurement wrapper for MPI_Reduce_local
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-2
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Reduce_local( void*        inbuf,
                  void*        inoutbuf,
                  int          count,
                  MPI_Datatype datatype,
                  MPI_Op       op )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_LOCAL ] );

        return_val = PMPI_Reduce_local( inbuf, inoutbuf, count, datatype, op );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REDUCE_LOCAL ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Reduce_local( inbuf, inoutbuf, count, datatype, op );
    }

    return return_val;
}
#endif

/**
 * @}
 */
