/*
 * This file is part of the SCOREP project (http://www.scorep.de)
 *
 * Copyright (c) 2009-2011,
 *    RWTH Aachen, Germany
 *    Gesellschaft fuer numerische Simulation mbH Braunschweig, Germany
 *    Technische Universitaet Dresden, Germany
 *    University of Oregon, Eugene USA
 *    Forschungszentrum Juelich GmbH, Germany
 *    Technische Universitaet Muenchen, Germany
 *
 * See the COPYING file in the package base directory for details.
 *
 */


/**
 * @file  SCOREP_Mpi_P2p.c
 * @maintainer Daniel Lorenz <d.lorenz@fz-juelich.de>
 * @status     alpha
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for point-to-point communication
 */

#include <config.h>
#include "SCOREP_Mpi.h"

/**
 * @name Blocking
 * @{
 */
#if HAVE( DECL_PMPI_BSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Bsend )
/**
 * Measurement wrapper for MPI_Bsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'me)'
 * @li MPI send event
 * @li exit region 'me)'
 */
int
MPI_Bsend( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          dest,
           int          tag,
           MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Bsend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Bsend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Bsend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_RSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Rsend )
/**
 * Measurement wrapper for MPI_Rsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'me)'
 * @li MPI send event
 * @li exit region 'me)'
 */
int
MPI_Rsend( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          dest,
           int          tag,
           MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Rsend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Rsend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Rsend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Send )
/**
 * Measurement wrapper for MPI_Send
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'me)'
 * @li MPI send event
 * @li exit region 'me)'
 */
int
MPI_Send( void*        buf,
          int          count,
          MPI_Datatype datatype,
          int          dest,
          int          tag,
          MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Send( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Send( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Send( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_SSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ssend )
/**
 * Measurement wrapper for MPI_Ssend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpSend.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'me)'
 * @li MPI send event
 * @li exit region 'me)'
 */
int
MPI_Ssend( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          dest,
           int          tag,
           MPI_Comm     comm )
{
    int      return_val;
    uint64_t start_time_stamp;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;

        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }
        return_val = PMPI_Ssend( buf, count, datatype, dest, tag, comm );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Ssend( buf, count, datatype, dest, tag, comm, start_time_stamp, return_val );
        }
    #endif

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Ssend( buf, count, datatype, dest, tag, comm );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_RECV ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Recv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int
MPI_Recv( void*        buf,
          int          count,
          MPI_Datatype datatype,
          int          source,
          int          tag,
          MPI_Comm     comm,
          MPI_Status*  status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sz;
        uint64_t   start_time_stamp;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RECV ] );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            start_time_stamp = SCOREP_GetLastTimeStamp();
        }
    #endif

        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }
        return_val = PMPI_Recv( buf, count, datatype, source, tag, comm, status );

    #if !defined( SCOREP_MPI_NO_HOOKS )
        if ( SCOREP_IS_MPI_HOOKS_ON )
        {
            SCOREP_Hooks_Post_MPI_Recv( buf, count, datatype, source, tag, comm, status, start_time_stamp, return_val );
        }
    #endif

        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Get_count( status, datatype, &count );
            SCOREP_MpiRecv( SCOREP_MPI_RANK_TO_PE( status->MPI_SOURCE, comm ),
                            SCOREP_MPI_COMM_ID( comm ), status->MPI_TAG, count * sz );
        }



        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RECV ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Recv( buf, count, datatype, source, tag, comm, status );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_PROBE ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Probe )
/**
 * Measurement wrapper for MPI_Probe
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Probe( int         source,
           int         tag,
           MPI_Comm    comm,
           MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_PROBE ] );

        return_val = PMPI_Probe( source, tag, comm, status );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_PROBE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Probe( source, tag, comm, status );
    }

    return return_val;
}
#endif


#if HAVE( DECL_PMPI_SENDRECV ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Sendrecv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI send event
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int
MPI_Sendrecv( void*        sendbuf,
              int          sendcount,
              MPI_Datatype sendtype,
              int          dest,
              int          sendtag,
              void*        recvbuf,
              int          recvcount,
              MPI_Datatype recvtype,
              int          source,
              int          recvtag,
              MPI_Comm     comm,
              MPI_Status*  status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sendsz, recvsz;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( sendtype, &sendsz );
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            sendtag, sendcount * sendsz );
        }

        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }

        return_val = PMPI_Sendrecv( sendbuf, sendcount, sendtype, dest,   sendtag,
                                    recvbuf, recvcount, recvtype, source, recvtag,
                                    comm, status );
        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            PMPI_Type_size( recvtype, &recvsz );
            PMPI_Get_count( status, recvtype, &recvcount );
            SCOREP_MpiRecv( SCOREP_MPI_RANK_TO_PE( status->MPI_SOURCE, comm ),
                            SCOREP_MPI_COMM_ID( comm ), status->MPI_TAG, recvcount * recvsz );
        }

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Sendrecv( sendbuf, sendcount, sendtype, dest,   sendtag,
                                    recvbuf, recvcount, recvtype, source, recvtag,
                                    comm, status );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_SENDRECV_REPLACE ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Sendrecv_replace
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Sequence of events:
 * @li enter region 'MPI_Recv'
 * @li MPI send event
 * @li MPI recv event
 * @li exit region 'MPI_Recv'
 */
int
MPI_Sendrecv_replace( void*        buf,
                      int          count,
                      MPI_Datatype datatype,
                      int          dest,
                      int          sendtag,
                      int          source,
                      int          recvtag,
                      MPI_Comm     comm,
                      MPI_Status*  status )
{
    int return_val;


    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int        sz;
        MPI_Status mystatus;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV_REPLACE ] );

        PMPI_Type_size( datatype, &sz );
        if ( dest != MPI_PROC_NULL )
        {
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ),
                            SCOREP_MPI_COMM_ID( comm ),
                            sendtag,
                            count * sz );
        }

        if ( status == MPI_STATUS_IGNORE )
        {
            status = &mystatus;
        }

        return_val = PMPI_Sendrecv_replace( buf, count, datatype, dest,
                                            sendtag, source, recvtag,
                                            comm, status );
        if ( source != MPI_PROC_NULL && return_val == MPI_SUCCESS )
        {
            SCOREP_MpiRecv( SCOREP_MPI_RANK_TO_PE( status->MPI_SOURCE, comm ),
                            SCOREP_MPI_COMM_ID( comm ), status->MPI_TAG, count * sz );
        }

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SENDRECV_REPLACE ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Sendrecv_replace( buf, count, datatype, dest,
                                            sendtag, source, recvtag,
                                            comm, status );
    }

    return return_val;
}
#endif

/**
 * @}
 * @name Non-blocking
 * @{
 */

#if HAVE( DECL_PMPI_IBSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ibsend )
/**
 * Measurement wrapper for MPI_Ibsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Ibsend( void*        buf,
            int          count,
            MPI_Datatype datatype,
            int          dest,
            int          tag,
            MPI_Comm     comm,
            MPI_Request* request )
{
    int return_val;
/*
   const int xnb_active = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;
/*
    uint32_t reqid = scorep_get_request_id();
 */
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IBSEND ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
/*
      if (xnb_active)
        scorep_attr_ui4(ELG_ATTR_REQUEST, reqid);
 */
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }

        return_val = PMPI_Ibsend( buf, count, datatype, dest, tag, comm, request );
/*
    if (xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
       scorep_request_create(*request, ERF_SEND,
                           tag, dest, count*sz, datatype, comm, reqid);
    }
 */
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IBSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Ibsend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_IRSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Irsend )
/**
 * Measurement wrapper for MPI_Irsend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Irsend( void*        buf,
            int          count,
            MPI_Datatype datatype,
            int          dest,
            int          tag,
            MPI_Comm     comm,
            MPI_Request* request )
{
    int return_val;
/*
   const int xnb_active = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;
/*
    uint32_t reqid = scorep_get_request_id();
 */
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IRSEND ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
/*
      if (xnb_active)
        scorep_attr_ui4(ELG_ATTR_REQUEST, reqid);
 */
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }

        return_val = PMPI_Irsend( buf, count, datatype, dest, tag, comm, request );
/*
    if (xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
       scorep_request_create(*request, ERF_SEND,
                           tag, dest, count*sz, datatype, comm, reqid);
    }
 */
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IRSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Irsend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ISEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Isend )
/**
 * Measurement wrapper for MPI_Isend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Isend( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          dest,
           int          tag,
           MPI_Comm     comm,
           MPI_Request* request )
{
    int return_val;
/*
   const int xnb_active = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;
/*
    uint32_t reqid = scorep_get_request_id();
 */
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ISEND ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
/*
      if (xnb_active)
        scorep_attr_ui4(ELG_ATTR_REQUEST, reqid);
 */
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }

        return_val = PMPI_Isend( buf, count, datatype, dest, tag, comm, request );
/*
    if (xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
       scorep_request_create(*request, ERF_SEND,
                           tag, dest, count*sz, datatype, comm, reqid);
    }
 */
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ISEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Isend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif
#if HAVE( DECL_PMPI_ISSEND ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Issend )
/**
 * Measurement wrapper for MPI_Issend
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_PtpIsend.w
 * @note C interface
 * @note Introduced with MPI 1
 * @ingroup p2p
 */
int
MPI_Issend( void*        buf,
            int          count,
            MPI_Datatype datatype,
            int          dest,
            int          tag,
            MPI_Comm     comm,
            MPI_Request* request )
{
    int return_val;
/*
   const int xnb_active = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        int sz;
/*
    uint32_t reqid = scorep_get_request_id();
 */
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_ISSEND ] );

        if ( dest != MPI_PROC_NULL )
        {
            PMPI_Type_size( datatype, &sz );
/*
      if (xnb_active)
        scorep_attr_ui4(ELG_ATTR_REQUEST, reqid);
 */
            SCOREP_MpiSend( SCOREP_MPI_RANK_TO_PE( dest, comm ), SCOREP_MPI_COMM_ID( comm ),
                            tag, count * sz );
        }

        return_val = PMPI_Issend( buf, count, datatype, dest, tag, comm, request );
/*
    if (xnb_active && dest != MPI_PROC_NULL && return_val == MPI_SUCCESS)
    {
       scorep_request_create(*request, ERF_SEND,
                           tag, dest, count*sz, datatype, comm, reqid);
    }
 */
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_ISSEND ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Issend( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_IRECV ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Irecv
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Irecv( void*        buf,
           int          count,
           MPI_Datatype datatype,
           int          source,
           int          tag,
           MPI_Comm     comm,
           MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
/* no asynchroneous communication handling at first
   const int xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    int return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IRECV ] );
    }

    return_val = PMPI_Irecv( buf, count, datatype, source, tag, comm, request );

/* no asynchroneous communication handling at first
   if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
   {
    uint32_t reqid = scorep_get_request_id();
    int sz;
    PMPI_Type_size(datatype, &sz);

    if (event_gen_active && xnb_active)
      scorep_mpi_recv_request(reqid);

    scorep_request_create(*request, ERF_RECV,
                       tag, 0, count * sz, datatype, comm, regid);
   }
 */

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IRECV ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_IPROBE ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Iprobe )
/**
 * Measurement wrapper for MPI_Iprobe
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Iprobe( int         source,
            int         tag,
            MPI_Comm    comm,
            int*        flag,
            MPI_Status* status )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_IPROBE ] );

        return_val = PMPI_Iprobe( source, tag, comm, flag, status );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_IPROBE ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Iprobe( source, tag, comm, flag, status );
    }

    return return_val;
}
#endif


#if HAVE( DECL_PMPI_WAIT ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Wait
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Wait( MPI_Request* request,
          MPI_Status*  status )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   MPI_Status         mystatus;
   struct ScorepRequest* orig_req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAIT ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (status == MPI_STATUS_IGNORE)
   {
    status = &mystatus;
   }

   orig_req   = scorep_request_get(*request);
 */
    return_val = PMPI_Wait( request, status );

/* no asynchroneous communication handling at the beginning included
   scorep_check_request(orig_req, status);
 */

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAIT ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitall( int          count,
             MPI_Request* requests,
             MPI_Status*  array_of_statuses )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
/* no asynchroneous communication handling at the beginning included
   struct ScorepRequest* orig_req;
   int                 i;
 */
    int return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITALL ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (array_of_statuses == MPI_STATUSES_IGNORE)
   {
    array_of_statuses = scorep_get_status_array(count);
   }
   scorep_save_request_array(requests, count);
 */
    return_val = PMPI_Waitall( count, requests, array_of_statuses );
/* no asynchroneous communication handling at the beginning included
   for (i = 0; i < count; i++)
   {
    orig_req = scorep_saved_request_get(i);
    scorep_check_request(orig_req, &(array_of_statuses[i]));
   }
 */
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITANY ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitany( int          count,
             MPI_Request* requests,
             int*         index,
             MPI_Status*  status )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   struct ScorepRequest* orig_req;
   MPI_Status         mystatus;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITANY ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (status == MPI_STATUS_IGNORE)
   {
    status = &mystatus;
   }

   scorep_save_request_array(requests, count);
 */
    return_val = PMPI_Waitany( count, requests, index, status );

/* no asynchroneous communication handling at the beginning included
   if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; ++i) {
        orig_req = scorep_saved_request_get(i);

        if (i == *index)
          scorep_check_request(orig_req, status);
        else if (orig_req)
          scorep_mpi_request_tested(orig_req->id);
      }
    }
   else
    {
      orig_req   = scorep_saved_request_get(*index);
      scorep_check_request(orig_req, status);
    }

 */
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITANY ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_WAITSOME ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Waitsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Waitsome( int          incount,
              MPI_Request* array_of_requests,
              int*         outcount,
              int*         array_of_indices,
              MPI_Status*  array_of_statuses )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   int                i;
   struct ScorepRequest* orig_req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_WAITSOME ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (array_of_statuses == MPI_STATUSES_IGNORE)
   {
    array_of_statuses = scorep_get_status_array(incount);
   }
   scorep_save_request_array(array_of_requests, incount);
 */
    return_val = PMPI_Waitsome( incount, array_of_requests, outcount,
                                array_of_indices, array_of_statuses );
/* no asynchroneous communication handling at the beginning included
   if (event_gen_active && xnb_active)
    {
      int j, tmp, cur;
      MPI_Status tmpstat;

      cur = 0;

      for (i = 0; i < incount; ++i)
        {
          orig_req = scorep_saved_request_get(i);

          if (orig_req)
            {
              for (j = cur; j < *outcount && i != array_of_indices[j]; ++j)
                ;

              if (j < *outcount)
                {
                  tmpstat               = array_of_statuses[cur];
                  scorep_check_request(orig_req, &(array_of_statuses[cur]));
                  array_of_statuses[j]  = tmpstat;

                  tmp                   = array_of_indices[cur];
                  array_of_indices[cur] = array_of_indices[j];
                  array_of_indices[j]   = tmp;

 ++cur;
                }
              else
                {
                  scorep_mpi_request_tested(orig_req->id);
                }
            }
        }
    }
   else
    {
      for (i=0; i<*outcount; ++i)
        {
          orig_req = scorep_saved_request_get(array_of_indices[i]);
          scorep_check_request(orig_req, &(array_of_statuses[i]));
        }
    }

 */

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_WAITSOME ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TEST ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Test
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Test( MPI_Request* request,
          int*         flag,
          MPI_Status*  status )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   struct ScorepRequest* orig_req;
   MPI_Status         mystatus;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TEST ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (status == MPI_STATUS_IGNORE)
   {
    status = &mystatus;
   }
   orig_req   = scorep_request_get(*request);
 */
    return_val = PMPI_Test( request, flag, status );
/* no asynchroneous communication handling at the beginning included
   if (*flag)
    {
      scorep_check_request(orig_req, status);
    }
   else if (orig_req && event_gen_active && xnb_active)
    {
      scorep_mpi_request_tested(orig_req->id);
    }
 */

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TEST ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTANY ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testany
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testany( int          count,
             MPI_Request* array_of_requests,
             int*         index,
             int*         flag,
             MPI_Status*  status )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   struct ScorepRequest* orig_req;
   MPI_Status         mystatus;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTANY ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (status == MPI_STATUS_IGNORE)
   {
    status = &mystatus;
   }
   scorep_save_request_array(array_of_requests, count);
 */
    return_val = PMPI_Testany( count, array_of_requests, index, flag, status );

/* no asynchroneous communication handling at the beginning included
   if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; ++i) {
        orig_req = scorep_saved_request_get(i);

        if (*index == i)
          scorep_check_request(orig_req, status);
        else if (orig_req)
          scorep_mpi_request_tested(orig_req->id);
      }
    }
   else if (*flag && *index != MPI_UNDEFINED)
    {
      orig_req = scorep_saved_request_get(*index);
      scorep_check_request(orig_req, status);
    }
 */
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTANY ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testall( int          count,
             MPI_Request* array_of_requests,
             int*         flag,
             MPI_Status*  array_of_statuses )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   int                i;
   struct ScorepRequest* orig_req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTALL ] );
    }
/* no asynchroneous communication handling at the beginning included
   if (array_of_statuses == MPI_STATUSES_IGNORE)
   {
    array_of_statuses = scorep_get_status_array(count);
   }
   scorep_save_request_array(array_of_requests, count);
 */
    return_val = PMPI_Testall( count, array_of_requests, flag, array_of_statuses );
/* no asynchroneous communication handling at the beginning included
   if (*flag)
    {
      for (i = 0; i < count; i++)
        {
          orig_req = scorep_saved_request_get(i);
          scorep_check_request(orig_req, &(array_of_statuses[i]));
        }
    }
   else if (event_gen_active && xnb_active)
    {
      int i;

      for (i = 0; i < count; i++)
        {
          orig_req = scorep_saved_request_get(i);
          if (orig_req)
            scorep_mpi_request_tested(orig_req->id);
        }
    }
 */
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TESTSOME ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Testsome
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Testsome( int          incount,
              MPI_Request* array_of_requests,
              int*         outcount,
              int*         array_of_indices,
              MPI_Status*  array_of_statuses )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
   int                i;
   struct ScorepRequest* orig_req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TESTSOME ] );
    }

/* no asynchroneous communication handling at the beginning included
   if (array_of_statuses == MPI_STATUSES_IGNORE)
   {
    array_of_statuses = scorep_get_status_array(incount);
   }
   scorep_save_request_array(array_of_requests, incount);
 */
    return_val = PMPI_Testsome( incount, array_of_requests, outcount,
                                array_of_indices, array_of_statuses );

/* no asynchroneous communication handling at the beginning included
   if (event_gen_active && xnb_active)
    {
      int cur, j, tmp;
      MPI_Status tmpstat;

      cur = 0;

      for (i=0; i<incount; ++i)
        {
          orig_req = scorep_saved_request_get(i);

          if (orig_req)
            {
              for (j = cur; j < *outcount && i != array_of_indices[j]; ++j)
                ;

              if (j < *outcount)
                {
                  tmpstat               = array_of_statuses[cur];
                  scorep_check_request(orig_req, &(array_of_statuses[cur]));
                  array_of_statuses[j]  = tmpstat;

                  tmp                   = array_of_indices[cur];
                  array_of_indices[cur] = array_of_indices[j];
                  array_of_indices[j]   = tmp;

 ++cur;
                }
              else
                {
                  scorep_mpi_request_tested(orig_req->id);
                }
            }
        }
    }
   else
    {
      for (i=0; i<*outcount; ++i)
        {
          orig_req = scorep_saved_request_get(array_of_indices[i]);
          scorep_check_request(orig_req, &(array_of_statuses[i]));
        }
    }

 */
    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TESTSOME ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

/**
 * @}
 * @name Persitent requests
 * @{
 */

/* no asynchroneous communication handling at the beginning included
   pragma wrapgen multiple regex(MPI_(S|B|R)[s]?end_init$) skel/SCOREP_Mpi_PtpSendinit.w
 */
#if HAVE( DECL_PMPI_BSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Bsend_init )
/**
 * Measurement wrapper for MPI_Bsend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Bsend_init( void*        buf,
                int          count,
                MPI_Datatype datatype,
                int          dest,
                int          tag,
                MPI_Comm     comm,
                MPI_Request* request )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND_INIT ] );

        return_val = PMPI_Bsend_init( buf, count, datatype, dest, tag, comm, request );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Bsend_init( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_RSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Rsend_init )
/**
 * Measurement wrapper for MPI_Rsend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Rsend_init( void*        buf,
                int          count,
                MPI_Datatype datatype,
                int          dest,
                int          tag,
                MPI_Comm     comm,
                MPI_Request* request )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND_INIT ] );

        return_val = PMPI_Rsend_init( buf, count, datatype, dest, tag, comm, request );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Rsend_init( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_SEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Send_init )
/**
 * Measurement wrapper for MPI_Send_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Send_init( void*        buf,
               int          count,
               MPI_Datatype datatype,
               int          dest,
               int          tag,
               MPI_Comm     comm,
               MPI_Request* request )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SEND_INIT ] );

        return_val = PMPI_Send_init( buf, count, datatype, dest, tag, comm, request );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Send_init( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_SSEND_INIT ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Ssend_init )
/**
 * Measurement wrapper for MPI_Ssend_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Ssend_init( void*        buf,
                int          count,
                MPI_Datatype datatype,
                int          dest,
                int          tag,
                MPI_Comm     comm,
                MPI_Request* request )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND_INIT ] );

        return_val = PMPI_Ssend_init( buf, count, datatype, dest, tag, comm, request );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_SSEND_INIT ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Ssend_init( buf, count, datatype, dest, tag, comm, request );
    }

    return return_val;
}
#endif


#if HAVE( DECL_PMPI_RECV_INIT ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Recv_init
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Recv_init( void*        buf,
               int          count,
               MPI_Datatype datatype,
               int          source,
               int          tag,
               MPI_Comm     comm,
               MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_RECV_INIT ] );
    }

    return_val = PMPI_Recv_init( buf, count, datatype, source, tag, comm, request );
/* no asynchroneous communication handling at the beginning included
   if (source != MPI_PROC_NULL && return_val == MPI_SUCCESS)
   {
    int sz;
    PMPI_Type_size(datatype, &sz);
    scorep_request_create(*request, (ERF_RECV | ERF_IS_PERSISTENT),
                       tag, source, count * sz, datatype, comm,
                       scorep_get_request_id());
   }
 */

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_RECV_INIT ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_START ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Start
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Start( MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    int return_val;

    if ( event_gen_active )
    {
        struct ScorepRequest* req;

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_START ] );

/* no asynchroneous communication handling at the beginning included
    req = scorep_request_get(*request);
    if (req && (req->flags & ERF_IS_PERSISTENT))
      {
        req->flags |= ERF_IS_ACTIVE;
        if ((req->flags & ERF_SEND) && (req->dest != MPI_PROC_NULL))
          {
            if (xnb_active)
              scorep_attr_ui4(ELG_ATTR_REQUEST, req->id);

            SCOREP_MpiSend(SCOREP_MPI_RANK_TO_PE(req->dest, req->comm),
                         SCOREP_COMM_ID(req->comm), req->tag,  req->bytes);
          }
        else if (req->flags & ERF_RECV && xnb_active)
          {
            scorep_mpi_recv_request(req->id);
          }
      }
 */
    }

    return_val = PMPI_Start( request );

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_START ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_STARTALL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Startall
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Startall( int          count,
              MPI_Request* array_of_requests )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    int return_val, i;

    if ( event_gen_active )
    {
        MPI_Request* request;

/* no asynchroneous communication handling at the beginning included
    struct ScorepRequest* req;
 */

        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_STARTALL ] );

/* no asynchroneous communication handling at the beginning included
    for (i = 0; i < count; i++)
    {
      request = &array_of_requests[i];
      req     = scorep_request_get(*request);

      if (req && (req->flags & ERF_IS_PERSISTENT))
        {
          req->flags |= ERF_IS_ACTIVE;
          if ((req->flags & ERF_SEND) && (req->dest != MPI_PROC_NULL))
            {
              if (xnb_active)
                scorep_attr_ui4(ELG_ATTR_REQUEST, req->id);

              SCOREP_MpiSend(SCOREP_MPI_RANK_TO_PE(req->dest, req->comm),
                           SCOREP_COMM_ID(req->comm), req->tag,  req->bytes);
            }
          else if (req->flags & ERF_RECV && xnb_active)
            {
              scorep_mpi_recv_request(req->id);
            }
        }

    }
 */
    }

    return_val = PMPI_Startall( count, array_of_requests );

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_STARTALL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_REQUEST_FREE ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Request_free
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Request_free( MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
/* no asynchroneous communication handling at the beginning included
   const int          xnb_active       = (scorep_mpi_enabled & SCOREP_MPI_ENABLED_XNONBLOCK);
 */
    int orig_req_null = ( *request == MPI_REQUEST_NULL );
    int return_val;
/* no asynchroneous communication handling at the beginning included
   struct ScorepRequest* req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_REQUEST_FREE ] );
    }

/* no asynchroneous communication handling at the beginning included
   req = scorep_request_get(*request);
   if (req)
    {
      if (req->flags & ERF_CAN_CANCEL && event_gen_active && xnb_active)
        {
          MPI_Status status;
          int        cancelled;
 */
    /* -- Must check if request was cancelled and write the
     *    cancel event. Not doing so will confuse the trace
     *    analysis.
     */
/*
          return_val = PMPI_Wait(request, &status);
          PMPI_Test_cancelled(&status, &cancelled);

          if (cancelled)
            esd_mpi_cancelled(req->id);
        }

      if ((req->flags & ERF_IS_PERSISTENT) && (req->flags & ERF_IS_ACTIVE))
 */        /* mark active requests for deallocation */
/*        req->flags |= ERF_DEALLOCATE;
      else
 */        /* deallocate inactive requests -*/
/*        epk_request_free(req);
    }
 */
/* -- We had to call PMPI_Wait for cancellable requests, which already
 *    frees (non-persistent) requests itself and sets them to
 *    MPI_REQUEST_NULL.
 *    As MPI_Request_free does not really like being called with
 *    MPI_REQUEST_NULL, we have to catch this situation here and only
 *    pass MPI_REQUEST_NULL if the application explicitely wanted that
 *    for some reason.
 */
    if ( *request != MPI_REQUEST_NULL || orig_req_null )
    {
        return_val = PMPI_Request_free( request );
    }


    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_REQUEST_FREE ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_CANCEL ) && !defined( SCOREP_MPI_NO_P2P )
/**
 * Measurement wrapper for MPI_Cancel
 * @note Manually adapted wrapper
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 */
int
MPI_Cancel( MPI_Request* request )
{
    const int event_gen_active = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P );
    int       return_val;
/* no asynchroneous communication handling at the beginning included
   struct ScorepRequest* req;
 */

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();

        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_CANCEL ] );
    }

    /* Mark request as cancellable and check for successful cancellation
     * on request completion or MPI_Request_free.
     * If XNONBLOCK is enabled, there will be a 'cancelled' event
     * instead of a normal completion event in the trace, which can be
     * checked for by the trace analysis.
     */

/* no asynchroneous communication handling at the beginning included
   req = scorep_request_get(*request);

   if (req)
    req->flags |= ERF_CAN_CANCEL;
 */
    return_val = PMPI_Cancel( request );

    if ( event_gen_active )
    {
        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_CANCEL ] );

        SCOREP_MPI_EVENT_GEN_ON();
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_TEST_CANCELLED ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Test_cancelled )
/**
 * Measurement wrapper for MPI_Test_cancelled
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Test_cancelled( MPI_Status* status,
                    int*        flag )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_TEST_CANCELLED ] );

        return_val = PMPI_Test_cancelled( status, flag );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_TEST_CANCELLED ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Test_cancelled( status, flag );
    }

    return return_val;
}
#endif


/**
 * @}
 * @name Auxiluary functions
 * @{
 */

#if HAVE( DECL_PMPI_BUFFER_ATTACH ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Buffer_attach )
/**
 * Measurement wrapper for MPI_Buffer_attach
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Buffer_attach( void* buffer,
                   int   size )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_ATTACH ] );

        return_val = PMPI_Buffer_attach( buffer, size );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_ATTACH ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Buffer_attach( buffer, size );
    }

    return return_val;
}
#endif

#if HAVE( DECL_PMPI_BUFFER_DETACH ) && !defined( SCOREP_MPI_NO_EXTRA ) && !defined( SCOREP_MPI_NO_P2P ) && !defined( MPI_Buffer_detach )
/**
 * Measurement wrapper for MPI_Buffer_detach
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-1
 * @ingroup p2p
 * Triggers an enter and exit event.
 * It wraps the me) call with enter and exit events.
 */
int
MPI_Buffer_detach( void* buffer,
                   int*  size )
{
    int return_val;

    if ( SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_P2P ) )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        SCOREP_EnterRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_DETACH ] );

        return_val = PMPI_Buffer_detach( buffer, size );

        SCOREP_ExitRegion( scorep_mpi_regid[ SCOREP__MPI_BUFFER_DETACH ] );
        SCOREP_MPI_EVENT_GEN_ON();
    }
    else
    {
        return_val = PMPI_Buffer_detach( buffer, size );
    }

    return return_val;
}
#endif


/**
 * @}
 */
